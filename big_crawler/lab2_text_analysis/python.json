{"paragraphs":[{"text":"%pyspark\ntext_file = sc.wholeTextFiles(\"hdfs:/user/zeppelin/wackerdetxt\").toDF()\n#text_file = sc.textFile(\"hdfs:/user/zeppelin/text3.txt\")","user":"user1","dateUpdated":"2018-04-30T12:35:06+0000","config":{"lineNumbers":true,"editorSetting":{"language":"python","editOnDblClick":false},"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1525072853061_1050951044","id":"20180426-090355_959351034","dateCreated":"2018-04-30T07:20:53+0000","dateStarted":"2018-04-30T12:35:06+0000","dateFinished":"2018-04-30T12:35:06+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:5717"},{"text":"%pyspark\nfrom pyspark.sql import Row\nfrom pyspark.sql.functions import regexp_replace, col, udf\nfrom langdetect import detect\nfrom spacy.lemmatizer import Lemmatizer\nfrom spacy.lang.en import LOOKUP as enlook\nfrom spacy.lang.de import LOOKUP as delook\nfrom pyspark.sql.types import StringType\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover\nfrom pyspark.sql.functions import explode\n\n#remove special charakter\ndf2 = text_file.withColumn(\"_2\", regexp_replace(col(\"_2\"), \"[\\\"'./§$&+,:;=?@#–|'<>.^*()%!-]\", \"\"))\ndf = df2.withColumn(\"_2\", regexp_replace(col(\"_2\"), \"\\\\s{2,}\", \"\"))\n\n#detect the language of the text\nlanguage_detect = udf(lambda x: detect(x), returnType=StringType())\ndf3 = df.withColumn(\"lang\",language_detect('_2'))\n\ndf3.show()\n\n# lemmatize the input\nlemmatizer = Lemmatizer(lookup=delook)\nlemmatizer1 = Lemmatizer(lookup=enlook)\ntokenizer = Tokenizer(inputCol=\"_2\", outputCol=\"words\")\ntokenized = tokenizer.transform(df3)\n\nlemma = udf(lambda x, lang: True if lang==\"de\" \" \".join([lemmatizer.lookup(i) for i in x]) else \" \".join([lemmatizer1.lookup(i) for i in x]) , returnType=StringType())\nlemmatized = tokenized.withColumn(\"stemmed\", lemma(col('words'), col('lang'))).drop('words').drop('_2')\n\n\n# remove stopwords\ntokenizer = Tokenizer(inputCol=\"stemmed\", outputCol=\"words\")\ntokenized = tokenizer.transform(lemmatized)\nremover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\nstopwords = remover.loadDefaultStopWords(\"german\") + remover.loadDefaultStopWords(\"english\")\nremover = remover.setStopWords(stopwords)\nnewDataSet = remover.transform(tokenized)\n\n# filter filtered words\ntest = newDataSet.withColumn(\"filtered\", explode(col(\"filtered\"))) \\\n  .groupBy(\"_1\", \"filtered\") \\\n  .agg(func.count(func.lit(1)).alias(\"count\")) \\\n  .sort(col(\"count\").desc())\n\n# wirte into db\ntest.write.mode(\"overwrite\").saveAsTable(\"de\")\n\ntest.show()\n\n","user":"user1","dateUpdated":"2018-04-30T12:35:07+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1525072853070_1049027300","id":"20180426-090408_940949723","dateCreated":"2018-04-30T07:20:53+0000","dateStarted":"2018-04-30T12:35:07+0000","dateFinished":"2018-04-30T12:35:35+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5718"},{"text":"%pyspark\n\nfrom pyspark.ml.feature import Tokenizer, RegexTokenizer\nfrom pyspark.sql.functions import col, udf\nfrom pyspark.sql.types import IntegerType\n\n\ntokenizer = Tokenizer(inputCol=\"line\", outputCol=\"words\")\n\n\ncountTokens = udf(lambda words: len(words), IntegerType())\n\ntokenized = tokenizer.transform(df)\ntokenized.select(\"line\", \"words\")\\\n    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)\n","dateUpdated":"2018-04-30T07:20:53+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1525072853070_1049027300","id":"20180426-090955_147800798","dateCreated":"2018-04-30T07:20:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5719"},{"text":"%pyspark\n\nfrom pyspark.ml.feature import StopWordsRemover\n\nremover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\nnewDataSet = remover.transform(tokenized) #.show(truncate=False)","dateUpdated":"2018-04-30T07:20:53+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1525072853072_1059030771","id":"20180426-092201_1062158944","dateCreated":"2018-04-30T07:20:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5720"},{"text":"%pyspark\n\nfrom sparknlp.annotator import *\n\n#helper = newDataSet.withColumn(\"filtered\", explode(col(\"filtered\"))).drop(\"line\").drop(\"words\")\n\nstemmer = PorterStemmer()\nstemmer.stem(newDataSet)\n","dateUpdated":"2018-04-30T07:20:53+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1525072853073_1058646022","id":"20180426-092548_1178709243","dateCreated":"2018-04-30T07:20:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5721"},{"text":"%pyspark\nimport pyspark.sql.functions as func\nfrom pyspark.sql.functions import split, explode\n\ntest = newDataSet.withColumn(\"filtered\", explode(col(\"filtered\"))) \\\n  .groupBy(\"filtered\") \\\n  .agg(func.count(func.lit(1)).alias(\"count\")) \\\n  .sort(col(\"count\").desc())\ntest.show()","dateUpdated":"2018-04-30T07:20:53+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1525072853073_1058646022","id":"20180426-092724_2067941535","dateCreated":"2018-04-30T07:20:53+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5722"},{"text":"%pyspark\r\nfrom spacy.lemmatizer import Lemmatizer\r\nfrom spacy.lang.en import LOOKUP as enlook\r\nfrom spacy.lang.de import LOOKUP as delook\r\nfrom pyspark.ml.feature import Tokenizer, StopWordsRemover\r\nfrom pyspark.sql.functions import explode\r\n\r\nlemmatizer = Lemmatizer(lookup=  enlook)\r\nv1 = lemmatizer.lookup(u'ducks')\r\n\r\nlemmatizer = Lemmatizer(lookup=  delook)\r\nv2 = lemmatizer.lookup(u'Aal')\r\n\r\nprint(v1)\r\nprint(v2)","user":"user1","dateUpdated":"2018-04-30T11:16:41+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1525072853074_1059800269","id":"20180426-110958_1211978531","dateCreated":"2018-04-30T07:20:53+0000","dateStarted":"2018-04-30T11:16:41+0000","dateFinished":"2018-04-30T11:16:41+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5723"},{"text":"%pyspark\n","user":"user1","dateUpdated":"2018-04-30T07:37:28+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1525073848432_913632913","id":"20180430-073728_774703948","dateCreated":"2018-04-30T07:37:28+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:5724"}],"name":"python","id":"2DD5FAHPY","angularObjects":{"2CHS8UYQQ:shared_process":[],"2CK8A9MEG:shared_process":[],"2C4U48MY3_spark2:shared_process":[],"2CKAY1A8Y:shared_process":[],"2CKEKWY8Z:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}