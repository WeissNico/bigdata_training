{"paragraphs":[{"text":"%pyspark\n\ntext_file = sc.textFile(\"hdfs:/user/zeppelin/text4.txt\")","user":"user1","dateUpdated":"2018-04-27T06:07:46+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python","lineNumbers":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1524733435220_1733892851","id":"20180426-090355_959351034","dateCreated":"2018-04-26T09:03:55+0000","dateStarted":"2018-04-27T06:07:46+0000","dateFinished":"2018-04-27T06:07:46+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:6306"},{"text":"%pyspark\nfrom pyspark.sql import Row\nfrom pyspark.sql.functions import regexp_replace, col\n\nlog_txt = text_file.filter(lambda line: line)\n#df = log_txt.toDF()  \n\nrow = Row(\"line\") # Or some other column name\ndf1 = log_txt.map(row).toDF()\n\ndf2 = df1.withColumn(\"line\", regexp_replace(col(\"line\"), \"[\\\"\\'./§$&+,:;=?@#|'<>.^*()%!-]\", \"\"))\ndf = df2.withColumn(\"line\", regexp_replace(col(\"line\"), \"\\\\s{2,}\", \"\"))\n\ndf.show()","user":"user1","dateUpdated":"2018-04-27T06:09:37+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1524733448029_1133916074","id":"20180426-090408_940949723","dateCreated":"2018-04-26T09:04:08+0000","dateStarted":"2018-04-27T06:09:37+0000","dateFinished":"2018-04-27T06:09:40+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6307"},{"text":"%pyspark\n\nfrom pyspark.ml.feature import Tokenizer, RegexTokenizer\nfrom pyspark.sql.functions import col, udf\nfrom pyspark.sql.types import IntegerType\n\n\ntokenizer = Tokenizer(inputCol=\"line\", outputCol=\"words\")\n\n\ncountTokens = udf(lambda words: len(words), IntegerType())\n\ntokenized = tokenizer.transform(df)\ntokenized.select(\"line\", \"words\")\\\n    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)\n","user":"user1","dateUpdated":"2018-04-27T06:09:47+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1524733795813_-1698329493","id":"20180426-090955_147800798","dateCreated":"2018-04-26T09:09:55+0000","dateStarted":"2018-04-27T06:09:47+0000","dateFinished":"2018-04-27T06:09:49+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6308"},{"text":"%pyspark\n\nfrom pyspark.ml.feature import StopWordsRemover\n\nremover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\nnewDataSet = remover.transform(tokenized) #.show(truncate=False)","user":"user1","dateUpdated":"2018-04-27T06:10:00+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1524734521035_1080555805","id":"20180426-092201_1062158944","dateCreated":"2018-04-26T09:22:01+0000","dateStarted":"2018-04-27T06:10:00+0000","dateFinished":"2018-04-27T06:10:00+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6309"},{"text":"%pyspark\n\nfrom sparknlp.annotator import *\n\n#helper = newDataSet.withColumn(\"filtered\", explode(col(\"filtered\"))).drop(\"line\").drop(\"words\")\n\nstemmer = PorterStemmer()\nstemmer.stem(newDataSet)\n","user":"user1","dateUpdated":"2018-04-26T13:14:40+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1524734748653_-866552754","id":"20180426-092548_1178709243","dateCreated":"2018-04-26T09:25:48+0000","dateStarted":"2018-04-26T13:14:40+0000","dateFinished":"2018-04-26T13:14:40+0000","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6310"},{"text":"%pyspark\nimport pyspark.sql.functions as func\nfrom pyspark.sql.functions import split, explode\n\ntest = newDataSet.withColumn(\"filtered\", explode(col(\"filtered\"))) \\\n  .groupBy(\"filtered\") \\\n  .agg(func.count(func.lit(1)).alias(\"count\")) \\\n  .sort(col(\"count\").desc())\ntest.show()","user":"user1","dateUpdated":"2018-04-27T06:12:24+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1524734844988_-1779407008","id":"20180426-092724_2067941535","dateCreated":"2018-04-26T09:27:24+0000","dateStarted":"2018-04-27T06:12:24+0000","dateFinished":"2018-04-27T06:12:27+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6311"},{"text":"%dep\n","user":"user1","dateUpdated":"2018-04-26T11:09:58+0000","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1524740998644_-349717918","id":"20180426-110958_1211978531","dateCreated":"2018-04-26T11:09:58+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:6312"}],"name":"python","id":"2DDJC2422","angularObjects":{"2CHS8UYQQ:shared_process":[],"2CK8A9MEG:shared_process":[],"2C4U48MY3_spark2:shared_process":[],"2CKAY1A8Y:shared_process":[],"2CKEKWY8Z:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}